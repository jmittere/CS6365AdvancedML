{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ad058d",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3922f1",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35076694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57dd73a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'label'], dtype='object')\n",
      "   id    label\n",
      "0   0  Dataset\n",
      "1   1  Dataset\n",
      "2   2  Dataset\n",
      "3   3  Dataset\n",
      "4   4  Dataset\n",
      "\n",
      "label\n",
      "Publication       2584\n",
      "ScienceKeyword    1609\n",
      "Dataset           1300\n",
      "Platform           142\n",
      "Instrument          83\n",
      "Project             44\n",
      "DataCenter           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "relationship_type\n",
      "HAS_SCIENCEKEYWORD    4015\n",
      "USES_DATASET          3623\n",
      "SUBCATEGORY_OF        1823\n",
      "HAS_PLATFORM          1519\n",
      "OF_PROJECT            1325\n",
      "HAS_DATASET           1300\n",
      "HAS_INSTRUMENT         215\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of training examples:  13820\n",
      "Number of validation examples:  860\n",
      "Number of test examples:  861\n"
     ]
    }
   ],
   "source": [
    "#import nodes and edges\n",
    "node_df = pd.read_csv('data/nodes.csv')\n",
    "edge_df = pd.read_csv('data/train_edges.csv')\n",
    "val_links = pd.read_csv('data/val_links.csv')\n",
    "test_links = pd.read_csv('data/test_links.csv')\n",
    "\n",
    "#print(node_df.head())\n",
    "#print(node_df['label'].unique())\n",
    "#print(edge_df.head())\n",
    "#print(edge_df['relationship_type'].unique())\n",
    "\n",
    "#node_df[\"properties\"] = node_df[\"properties\"].apply(ast.literal_eval)\n",
    "#print(type(node_df['properties'].iloc[0]))\n",
    "\n",
    "#drop properties column because won't be used\n",
    "node_df = node_df.drop('properties', axis=1)\n",
    "#remove quotes and brackets so labels are simpler\n",
    "node_df['label'] = node_df['label'].str.replace(\"['\", \"\")\n",
    "node_df['label'] = node_df['label'].str.replace(\"']\", \"\")\n",
    "print(node_df.columns)\n",
    "print(node_df.head())\n",
    "print()\n",
    "print(node_df['label'].value_counts())\n",
    "\n",
    "print()\n",
    "print(edge_df['relationship_type'].value_counts())\n",
    "\n",
    "print()\n",
    "print(\"Number of training examples: \", len(edge_df))\n",
    "print(\"Number of validation examples: \", len(val_links))\n",
    "print(\"Number of test examples: \", len(test_links))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a7604",
   "metadata": {},
   "source": [
    "**Important: - `val_links.csv`: Contains `HAS_SCIENCEKEYWORD` edges for validation.`test_links.csv`: Contains `HAS_SCIENCEKEYWORD` edges for testing.** <br>\n",
    "So, the validation and test sets are only looking at relationship type Dataset -> ScienceKeyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd02e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[5763, 1], edge_index=[2, 13820])\n"
     ]
    }
   ],
   "source": [
    "# Map node IDs to contiguous indices for PyG\n",
    "node_df['idx'] = range(len(node_df))\n",
    "id_to_idx = dict(zip(node_df['id'], node_df['idx']))\n",
    "\n",
    "ones = torch.ones((len(node_df), 1))  # dummy node features for each node\n",
    "# Map node IDs to indices\n",
    "edge_df['source'] = edge_df['source'].map(id_to_idx)\n",
    "edge_df['target'] = edge_df['target'].map(id_to_idx)\n",
    "\n",
    "val_links['source'] = val_links['source'].map(id_to_idx)\n",
    "val_links['target'] = val_links['target'].map(id_to_idx)\n",
    "\n",
    "test_links['source'] = test_links['source'].map(id_to_idx)\n",
    "test_links['target'] = test_links['target'].map(id_to_idx)\n",
    "\n",
    "\n",
    "# Create edge_index tensor\n",
    "edge_index = torch.tensor(edge_df[['source', 'target']].values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "data = Data(x=ones, edge_index=edge_index)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e39c0c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes:  5763\n",
      "Number of Node features:  1\n",
      "\n",
      "Number of Edges:  13820\n",
      "\n",
      "Has Isolated Nodes:  False\n",
      "Has Self Loops:  False\n"
     ]
    }
   ],
   "source": [
    "#Summary of Dataset Structure and Key Statistics for Part 1\n",
    "print(\"Number of Nodes: \", data.num_nodes)\n",
    "print(\"Number of Node features: \", data.num_node_features)\n",
    "print()\n",
    "print(\"Number of Edges: \", data.num_edges)\n",
    "print()\n",
    "print(\"Has Isolated Nodes: \" , data.has_isolated_nodes())\n",
    "print(\"Has Self Loops: \", data.has_self_loops())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240188c",
   "metadata": {},
   "source": [
    "## Part 2: Link Prediction\n",
    "### Method #1: Embedding-Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bc828",
   "metadata": {},
   "source": [
    "Task: Apply an embedding-based method for link prediction. ○ Description: Train a model that generates node embeddings, then use those embeddings to predict links. Print relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c16c65",
   "metadata": {},
   "source": [
    "\"The primary goal of the NASA Knowledge Graph is to bridge scientific publications with the datasets they reference, facilitating deeper insights and research opportunities within NASA's scientific and data ecosystem. By organizing these interconnections within a graph structure, this dataset enables advanced analyses, such as discovering influential datasets, understanding research trends, and exploring scientific collaborations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b34057",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/2.6.0/tutorial/shallow_node_embeddings.\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "721d7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2Vec(5763, 256)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = Node2Vec(data.edge_index, embedding_dim=256, walk_length=10, context_size=5, walks_per_node=10).to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "loader = model.loader(batch_size=32, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43cd7fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 141.0827\n",
      "Epoch 2, Loss: 141.0447\n",
      "Epoch 3, Loss: 141.0979\n",
      "Epoch 4, Loss: 141.1061\n",
      "Epoch 5, Loss: 140.9801\n",
      "Epoch 6, Loss: 141.1250\n",
      "Epoch 7, Loss: 141.0279\n",
      "Epoch 8, Loss: 141.1664\n",
      "Epoch 9, Loss: 141.0861\n",
      "Epoch 10, Loss: 141.2016\n",
      "Epoch 11, Loss: 141.0301\n",
      "Epoch 12, Loss: 141.0992\n",
      "Epoch 13, Loss: 141.0330\n",
      "Epoch 14, Loss: 141.0756\n",
      "Epoch 15, Loss: 140.9596\n",
      "Epoch 16, Loss: 141.1630\n",
      "Epoch 17, Loss: 141.0621\n",
      "Epoch 18, Loss: 140.9745\n",
      "Epoch 19, Loss: 140.9383\n",
      "Epoch 20, Loss: 141.0848\n",
      "Epoch 21, Loss: 140.9792\n",
      "Epoch 22, Loss: 140.8897\n",
      "Epoch 23, Loss: 140.8134\n",
      "Epoch 24, Loss: 140.8924\n",
      "Epoch 25, Loss: 141.1280\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw, neg_rw)  # negative samples are neg_rw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3aec2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Negative Edges\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_nodes = data.num_nodes\n",
    "\n",
    "def gen_neg_edges(negative_edges_goal, edge_index, num_nodes):\n",
    "    existing_edges = set([tuple(e) for e in edge_index.t().tolist()])\n",
    "    neg_edges = set()\n",
    "    while len(neg_edges) < negative_edges_goal:\n",
    "        u = np.random.randint(0, num_nodes)\n",
    "        v = np.random.randint(0, num_nodes)\n",
    "        if u == v: \n",
    "            continue  # skip self-loops\n",
    "        if (u,v) in existing_edges or (v,u) in existing_edges:\n",
    "            continue\n",
    "        neg_edges.add((u,v))\n",
    "    return np.array(list(neg_edges))\n",
    "\n",
    "val_neg = gen_neg_edges(len(val_links), data.edge_index, num_nodes)\n",
    "test_neg = gen_neg_edges(len(test_links), data.edge_index, num_nodes)\n",
    "\n",
    "# Combine positive and negative edges\n",
    "val_pos = val_links[['source','target']].values\n",
    "val_edges = np.vstack([val_pos, val_neg])\n",
    "val_labels = np.hstack([np.ones(len(val_pos)), np.zeros(len(val_neg))])\n",
    "\n",
    "test_pos = test_links[['source','target']].values\n",
    "test_edges = np.vstack([test_pos, test_neg])\n",
    "test_labels = np.hstack([np.ones(len(test_pos)), np.zeros(len(test_neg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28d1595c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.6644699837750134\n",
      "Test AUC: 0.6443821232637413\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "embeddings = model()\n",
    "\n",
    "def edge_score(u, v, emb):\n",
    "    return (emb[u] * emb[v]).sum().item()  # dot product\n",
    "\n",
    "val_scores = [edge_score(u, v, embeddings) for u,v in val_edges]\n",
    "test_scores = [edge_score(u, v, embeddings) for u,v in test_edges]\n",
    "\n",
    "#Evaluate with AUC\n",
    "val_auc = roc_auc_score(val_labels, val_scores)\n",
    "test_auc = roc_auc_score(test_labels, test_scores)\n",
    "\n",
    "print(\"Validation AUC:\", val_auc)\n",
    "print(\"Test AUC:\", test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376f921",
   "metadata": {},
   "source": [
    "### Method 2: Alternative Approach \n",
    "Task: Choose and implement another link prediction method. ○ Description: This method should not use embeddings. You can use any approach of your choice. Compare the performance of this method with the embedding-based method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
