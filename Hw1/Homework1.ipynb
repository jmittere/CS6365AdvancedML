{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ad058d",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3922f1",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35076694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd73a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'label'], dtype='object')\n",
      "   id    label\n",
      "0   0  Dataset\n",
      "1   1  Dataset\n",
      "2   2  Dataset\n",
      "3   3  Dataset\n",
      "4   4  Dataset\n",
      "\n",
      "label\n",
      "Publication       2584\n",
      "ScienceKeyword    1609\n",
      "Dataset           1300\n",
      "Platform           142\n",
      "Instrument          83\n",
      "Project             44\n",
      "DataCenter           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "relationship_type\n",
      "HAS_SCIENCEKEYWORD    4015\n",
      "USES_DATASET          3623\n",
      "SUBCATEGORY_OF        1823\n",
      "HAS_PLATFORM          1519\n",
      "OF_PROJECT            1325\n",
      "HAS_DATASET           1300\n",
      "HAS_INSTRUMENT         215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#import nodes and edges\n",
    "node_df = pd.read_csv('data/nodes.csv')\n",
    "#directed graph\n",
    "edge_df = pd.read_csv('data/train_edges.csv')\n",
    "\n",
    "#drop properties column because won't be used\n",
    "node_df = node_df.drop('properties', axis=1)\n",
    "#remove quotes and brackets so labels are simpler\n",
    "node_df['label'] = node_df['label'].str.replace(\"['\", \"\")\n",
    "node_df['label'] = node_df['label'].str.replace(\"']\", \"\")\n",
    "print(node_df.columns)\n",
    "print(node_df.head())\n",
    "print()\n",
    "print(node_df['label'].value_counts())\n",
    "\n",
    "print()\n",
    "print(edge_df['relationship_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd02e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "# Define node sets by label\n",
    "for label in node_df['label'].unique():\n",
    "    node_ids = node_df[node_df['label'] == label]['id'].values #get all nodes for each type of node\n",
    "    data[label].num_nodes = len(node_ids)\n",
    "    data[label].x = torch.ones((len(node_ids), 1))  # dummy node features for the given node type\n",
    "\n",
    "#for each edge relationship type\n",
    "for relationship_type, relationship_edges in edge_df.groupby('relationship_type'):\n",
    "    src_nodes_of_type = relationship_edges['source'].values\n",
    "    dst_nodes_of_type = relationship_edges['target'].values\n",
    "    src_ids = torch.tensor(src_nodes_of_type, dtype=torch.long)\n",
    "    dst_ids = torch.tensor(dst_nodes_of_type, dtype=torch.long)\n",
    "\n",
    "    # Determine node types dynamically\n",
    "    #print(relationship_edges['source'].iloc[0])\n",
    "    source_id = relationship_edges['source'].iloc[0]\n",
    "    src_type = node_df[node_df['id'] == source_id]['label'].values[0]\n",
    "\n",
    "    target_id = relationship_edges['target'].iloc[0]\n",
    "    dst_type = node_df[node_df['id'] == target_id]['label'].values[0]\n",
    "\n",
    "    #creates edge_index tensors for each edge relationship_type\n",
    "    # .edge_index tensor is of dimensions (2, NumEdges) matrix that PyG needs\n",
    "    # 2 rows where first row is Src ids and second row is Dst ids for edges\n",
    "    data[(src_type, relationship_type, dst_type)].edge_index = torch.stack([src_ids, dst_ids], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39c0c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes:  5763\n",
      "Number of Node features:  {'Dataset': 1, 'DataCenter': 1, 'Project': 1, 'Platform': 1, 'Instrument': 1, 'Publication': 1, 'ScienceKeyword': 1}\n",
      "Node Types:  ['Dataset', 'DataCenter', 'Project', 'Platform', 'Instrument', 'Publication', 'ScienceKeyword']\n",
      "\n",
      "Number of Edges:  13820\n",
      "Edge Types (Src node type, relationship_type, Dst node type):  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Platform', 'HAS_INSTRUMENT', 'Instrument'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Dataset', 'OF_PROJECT', 'Project'), ('ScienceKeyword', 'SUBCATEGORY_OF', 'ScienceKeyword'), ('Publication', 'USES_DATASET', 'Dataset')]\n",
      "\n",
      "Has Isolated Nodes:  False\n",
      "Has Self Loops:  False\n"
     ]
    }
   ],
   "source": [
    "#Summary of Dataset Structure and Key Statistics for Part 1\n",
    "print(\"Number of Nodes: \", data.num_nodes)\n",
    "print(\"Number of Node features: \", data.num_node_features)\n",
    "print(\"Node Types: \", data.node_types)\n",
    "print()\n",
    "print(\"Number of Edges: \", data.num_edges)\n",
    "print(\"Edge Types (Src node type, relationship_type, Dst node type): \",  data.edge_types)\n",
    "print()\n",
    "print(\"Has Isolated Nodes: \" , data.has_isolated_nodes())\n",
    "print(\"Has Self Loops: \", data.has_self_loops())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240188c",
   "metadata": {},
   "source": [
    "## Part 2: Link Prediction\n",
    "### Method #1: Embedding-Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bc828",
   "metadata": {},
   "source": [
    "Task: Apply an embedding-based method for link prediction. ○ Description: Train a model that generates node embeddings, then use those embeddings to predict links. Print relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c16c65",
   "metadata": {},
   "source": [
    "\"The primary goal of the NASA Knowledge Graph is to bridge scientific publications with the datasets they reference, facilitating deeper insights and research opportunities within NASA's scientific and data ecosystem. By organizing these interconnections within a graph structure, this dataset enables advanced analyses, such as discovering influential datasets, understanding research trends, and exploring scientific collaborations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbdfeba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 meta-paths starting and ending at 'Publication':\n",
      "Found 4 meta-paths starting and ending at 'ScienceKeyword':\n",
      "ScienceKeyword → ScienceKeyword\n",
      "ScienceKeyword → ScienceKeyword → ScienceKeyword\n",
      "ScienceKeyword → ScienceKeyword → ScienceKeyword → ScienceKeyword\n",
      "ScienceKeyword → ScienceKeyword → ScienceKeyword → ScienceKeyword → ScienceKeyword\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "def build_schema_graph(data: HeteroData):\n",
    "    \"\"\"\n",
    "    Build a directed schema graph (node types as nodes, relation types as edges)\n",
    "    from a PyG HeteroData object.\n",
    "    \"\"\"\n",
    "    schema = nx.DiGraph()\n",
    "    \n",
    "    for src_type, rel_type, dst_type in data.edge_types:\n",
    "        schema.add_edge(src_type, dst_type, relation=rel_type)\n",
    "    \n",
    "    return schema\n",
    "\n",
    "\n",
    "def find_metapaths(schema, start_type, max_length=5):\n",
    "    \"\"\"\n",
    "    Find all meta-paths in the schema graph that start and end\n",
    "    at the same node type.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "\n",
    "    def dfs(current, path):\n",
    "        if len(path) > max_length:\n",
    "            return\n",
    "        # if we've returned to the start type (and it's not the first node)\n",
    "        if len(path) > 1 and current == start_type:\n",
    "            paths.append(list(path))\n",
    "        for neighbor in schema.neighbors(current):\n",
    "            path.append(neighbor)\n",
    "            dfs(neighbor, path)\n",
    "            path.pop()\n",
    "\n",
    "    dfs(start_type, [start_type])\n",
    "    return paths\n",
    "\n",
    "\n",
    "# ---- Example usage ----\n",
    "# Assuming you already have your HeteroData object called `data`\n",
    "\n",
    "schema = build_schema_graph(data)\n",
    "\n",
    "\n",
    "metapaths = find_metapaths(schema, start_type='Publication', max_length=5)\n",
    "print(f\"Found {len(metapaths)} meta-paths starting and ending at 'Publication':\")\n",
    "for mp in metapaths:\n",
    "    print(\" → \".join(mp))\n",
    "\n",
    "metapaths = find_metapaths(schema, start_type='ScienceKeyword', max_length=5)\n",
    "print(f\"Found {len(metapaths)} meta-paths starting and ending at 'ScienceKeyword':\")\n",
    "for mp in metapaths:\n",
    "    print(\" → \".join(mp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce5ee9",
   "metadata": {},
   "source": [
    "Idea: If there were metapaths that started at Publication and ended at Publication, we could use these manually defined paths to look at Publications with similar embeddings and infer that the Publications were in the same topic area. If we can predict links between Publications and Projects that we think would be linked, then we can connect author's of the Publications with new Projects that should already be linked due to similar fields/common interests. <br>\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/2.6.0/tutorial/shallow_node_embeddings.\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86710bb3",
   "metadata": {},
   "source": [
    "Since there are no metapaths that start and end at the same node type besides the ScienceKeyword node type, I am going to use regular Node2Vec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "721d7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Nodes:  5763\n"
     ]
    }
   ],
   "source": [
    "#Generating Embeddings using Node2Vec package from PyG\n",
    "\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "homog_data = data.to_homogeneous()\n",
    "print(\"Num Nodes: \", homog_data.num_nodes)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = Node2Vec(homog_data.edge_index, embedding_dim=128, walk_length=10, context_size=5, walks_per_node=10, sparse=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd98ca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old num nodes: 29178\n",
      "New num nodes: 5763\n",
      "Node2Vec(5763, 128)\n"
     ]
    }
   ],
   "source": [
    "# Assuming your edges are in homo_data.edge_index\n",
    "edge_index = homog_data.edge_index\n",
    "\n",
    "# Step 1. Get unique node IDs actually used in the graph\n",
    "unique_nodes = torch.unique(edge_index)\n",
    "\n",
    "# Step 2. Build a mapping from old → new ID\n",
    "id_map = {old.item(): new for new, old in enumerate(unique_nodes)}\n",
    "\n",
    "# Step 3. Remap edge_index\n",
    "mapped_edges = torch.tensor([\n",
    "    [id_map[src.item()] for src in edge_index[0]],\n",
    "    [id_map[dst.item()] for dst in edge_index[1]]\n",
    "], dtype=torch.long)\n",
    "\n",
    "print(f\"Old num nodes: {edge_index.max().item() + 1}\")\n",
    "print(f\"New num nodes: {len(unique_nodes)}\")\n",
    "\n",
    "model = Node2Vec(mapped_edges, embedding_dim=128, walk_length=20, context_size=10, walks_per_node=10)\n",
    "print(model)\n",
    "\n",
    "loader = model.loader(batch_size=128, shuffle=True)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43cd7fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 5.3660\n",
      "Epoch: 002, Loss: 4.0846\n",
      "Epoch: 003, Loss: 3.1239\n",
      "Epoch: 004, Loss: 2.4048\n",
      "Epoch: 005, Loss: 1.9087\n",
      "Epoch: 006, Loss: 1.5544\n",
      "Epoch: 007, Loss: 1.3038\n",
      "Epoch: 008, Loss: 1.1337\n",
      "Epoch: 009, Loss: 1.0144\n",
      "Epoch: 010, Loss: 0.9336\n",
      "Epoch: 011, Loss: 0.8804\n",
      "Epoch: 012, Loss: 0.8434\n",
      "Epoch: 013, Loss: 0.8164\n",
      "Epoch: 014, Loss: 0.7996\n",
      "Epoch: 015, Loss: 0.7839\n",
      "Epoch: 016, Loss: 0.7740\n",
      "Epoch: 017, Loss: 0.7658\n",
      "Epoch: 018, Loss: 0.7596\n",
      "Epoch: 019, Loss: 0.7550\n",
      "Epoch: 020, Loss: 0.7506\n",
      "Epoch: 021, Loss: 0.7472\n",
      "Epoch: 022, Loss: 0.7448\n",
      "Epoch: 023, Loss: 0.7421\n",
      "Epoch: 024, Loss: 0.7405\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(1, 25):\n",
    "    loss = train()\n",
    "    #acc = test()\n",
    "    #print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Acc: {acc:.4f}')\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f3921de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2Vec(5763, 128)\n",
      "Missing source nodes: set()\n",
      "Missing target nodes: {nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 4120.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 4142.0, nan, nan, nan, 4149.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 3880.0, 3881.0, nan, 3939.0, 3941.0, 3963.0, nan, 4022.0, 4055.0}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -9223372036854775808 is out of bounds for dimension 0 with size 5763",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing target nodes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, missing_dst)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#compute dot products between src and dst node embeddings for similarity score\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m val_scores \u001b[38;5;241m=\u001b[39m (embeddings[val_src] \u001b[38;5;241m*\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_dst\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m (embeddings[test_src] \u001b[38;5;241m*\u001b[39m embeddings[test_dst])\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#wrap in sigmoid to get probabilities for each src and dst pair\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -9223372036854775808 is out of bounds for dimension 0 with size 5763"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "embeddings = model()\n",
    "print(model)\n",
    "\n",
    "#import validation and test edges\n",
    "val_links = pd.read_csv('data/val_links.csv')\n",
    "val_links['source'] = val_links['source'].map(id_map)\n",
    "val_links['target'] = val_links['target'].map(id_map)\n",
    "test_links = pd.read_csv('data/test_links.csv')\n",
    "test_links['source'] = test_links['source'].map(id_map)\n",
    "test_links['target'] = test_links['target'].map(id_map)\n",
    "\n",
    "#convert validation and test CSVs to tensors\n",
    "val_src = torch.tensor(val_links['source'].values, dtype=torch.long)\n",
    "val_dst = torch.tensor(val_links['target'].values, dtype=torch.long)\n",
    "\n",
    "test_src = torch.tensor(test_links['source'].values, dtype=torch.long)\n",
    "test_dst = torch.tensor(test_links['target'].values, dtype=torch.long)\n",
    "\n",
    "missing_src = set(val_links['source']) - set(id_map.keys())\n",
    "missing_dst = set(val_links['target']) - set(id_map.keys())\n",
    "print(\"Missing source nodes:\", missing_src)\n",
    "print(\"Missing target nodes:\", missing_dst)\n",
    "\n",
    "#compute dot products between src and dst node embeddings for similarity score\n",
    "val_scores = (embeddings[val_src] * embeddings[val_dst]).sum(dim=1)\n",
    "test_scores = (embeddings[test_src] * embeddings[test_dst]).sum(dim=1)\n",
    "\n",
    "#wrap in sigmoid to get probabilities for each src and dst pair\n",
    "val_probs = torch.sigmoid(val_scores)\n",
    "test_probs = torch.sigmoid(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376f921",
   "metadata": {},
   "source": [
    "### Method 2: Alternative Approach \n",
    "Task: Choose and implement another link prediction method. ○ Description: This method should not use embeddings. You can use any approach of your choice. Compare the performance of this method with the embedding-based method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
