{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ad058d",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1233033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "#Homework 1\n",
    "x = \"hello world\"\n",
    "print(\"hello world\")\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3922f1",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35076694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "57dd73a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'label'], dtype='object')\n",
      "   id    label\n",
      "0   0  Dataset\n",
      "1   1  Dataset\n",
      "2   2  Dataset\n",
      "3   3  Dataset\n",
      "4   4  Dataset\n",
      "\n",
      "label\n",
      "Publication       2584\n",
      "ScienceKeyword    1609\n",
      "Dataset           1300\n",
      "Platform           142\n",
      "Instrument          83\n",
      "Project             44\n",
      "DataCenter           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "relationship_type\n",
      "HAS_SCIENCEKEYWORD    4015\n",
      "USES_DATASET          3623\n",
      "SUBCATEGORY_OF        1823\n",
      "HAS_PLATFORM          1519\n",
      "OF_PROJECT            1325\n",
      "HAS_DATASET           1300\n",
      "HAS_INSTRUMENT         215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#import nodes and edges\n",
    "node_df = pd.read_csv('data/nodes.csv')\n",
    "#directed graph\n",
    "edge_df = pd.read_csv('data/train_edges.csv')\n",
    "#print(node_df.head())\n",
    "#print(node_df['label'].unique())\n",
    "#print(edge_df.head())\n",
    "#print(edge_df['relationship_type'].unique())\n",
    "\n",
    "#node_df[\"properties\"] = node_df[\"properties\"].apply(ast.literal_eval)\n",
    "#print(type(node_df['properties'].iloc[0]))\n",
    "\n",
    "#drop properties column because won't be used\n",
    "node_df = node_df.drop('properties', axis=1)\n",
    "#remove annoying quotes and brackets so labels are simpler\n",
    "node_df['label'] = node_df['label'].str.replace(\"['\", \"\")\n",
    "node_df['label'] = node_df['label'].str.replace(\"']\", \"\")\n",
    "print(node_df.columns)\n",
    "print(node_df.head())\n",
    "print()\n",
    "print(node_df['label'].value_counts())\n",
    "\n",
    "print()\n",
    "print(edge_df['relationship_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd02e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "# Define node sets by label\n",
    "for label in node_df['label'].unique():\n",
    "    node_ids = node_df[node_df['label'] == label]['id'].values #get all nodes for each type of node\n",
    "    data[label].num_nodes = len(node_ids)\n",
    "    data[label].x = torch.ones((len(node_ids), 1))  # dummy node features for the given node type\n",
    "\n",
    "#for each edge relationship type\n",
    "for relationship_type, relationship_edges in edge_df.groupby('relationship_type'):\n",
    "    src_nodes_of_type = relationship_edges['source'].values\n",
    "    dst_nodes_of_type = relationship_edges['target'].values\n",
    "    src_ids = torch.tensor(src_nodes_of_type, dtype=torch.long)\n",
    "    dst_ids = torch.tensor(dst_nodes_of_type, dtype=torch.long)\n",
    "\n",
    "    # Determine node types dynamically\n",
    "    #print(relationship_edges['source'].iloc[0])\n",
    "    source_id = relationship_edges['source'].iloc[0]\n",
    "    src_type = node_df[node_df['id'] == source_id]['label'].values[0]\n",
    "\n",
    "    target_id = relationship_edges['target'].iloc[0]\n",
    "    dst_type = node_df[node_df['id'] == target_id]['label'].values[0]\n",
    "\n",
    "    #creates edge_index tensors for each edge relationship_type\n",
    "    # .edge_index tensor is of dimensions (2, NumEdges) matrix that PyG needs\n",
    "    # 2 rows where first row is Src ids and second row is Dst ids for edges\n",
    "    data[(src_type, relationship_type, dst_type)].edge_index = torch.stack([src_ids, dst_ids], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e39c0c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes:  5763\n",
      "Number of Node features:  {'Dataset': 1, 'DataCenter': 1, 'Project': 1, 'Platform': 1, 'Instrument': 1, 'Publication': 1, 'ScienceKeyword': 1}\n",
      "Node Types:  ['Dataset', 'DataCenter', 'Project', 'Platform', 'Instrument', 'Publication', 'ScienceKeyword']\n",
      "\n",
      "Number of Edges:  13820\n",
      "Edge Types (Src node type, relationship_type, Dst node type):  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Platform', 'HAS_INSTRUMENT', 'Instrument'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Dataset', 'OF_PROJECT', 'Project'), ('ScienceKeyword', 'SUBCATEGORY_OF', 'ScienceKeyword'), ('Publication', 'USES_DATASET', 'Dataset')]\n",
      "\n",
      "Has Isolated Nodes:  False\n",
      "Has Self Loops:  False\n"
     ]
    }
   ],
   "source": [
    "#Summary of Dataset Structure and Key Statistics for Part 1\n",
    "print(\"Number of Nodes: \", data.num_nodes)\n",
    "print(\"Number of Node features: \", data.num_node_features)\n",
    "print(\"Node Types: \", data.node_types)\n",
    "print()\n",
    "print(\"Number of Edges: \", data.num_edges)\n",
    "print(\"Edge Types (Src node type, relationship_type, Dst node type): \",  data.edge_types)\n",
    "print()\n",
    "print(\"Has Isolated Nodes: \" , data.has_isolated_nodes())\n",
    "print(\"Has Self Loops: \", data.has_self_loops())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240188c",
   "metadata": {},
   "source": [
    "## Part 2: Link Prediction\n",
    "### Method #1: Embedding-Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bc828",
   "metadata": {},
   "source": [
    "Task: Apply an embedding-based method for link prediction. ○ Description: Train a model that generates node embeddings, then use those embeddings to predict links. Print relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c16c65",
   "metadata": {},
   "source": [
    "\"The primary goal of the NASA Knowledge Graph is to bridge scientific publications with the datasets they reference, facilitating deeper insights and research opportunities within NASA's scientific and data ecosystem. By organizing these interconnections within a graph structure, this dataset enables advanced analyses, such as discovering influential datasets, understanding research trends, and exploring scientific collaborations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fbdfeba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_metapaths_from_heterodata(data, max_depth=3):\n",
    "    \"\"\"\n",
    "    Generate all metapaths from a PyG HeteroData object using DFS.\n",
    "\n",
    "    Args:\n",
    "        data (HeteroData): The heterogeneous graph.\n",
    "        max_depth (int): Maximum number of edges in each metapath.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[list[tuple[str, str, str]]]]:\n",
    "            A dictionary mapping each starting node type to its metapaths.\n",
    "    \"\"\"\n",
    "    # Step 1: Build schema graph (node type -> [(rel, dst_type)])\n",
    "    graph = defaultdict(list)\n",
    "    for (src_type, rel_type, dst_type) in data.edge_types:\n",
    "        graph[src_type].append((rel_type, dst_type))\n",
    "\n",
    "    # Step 2: DFS to enumerate all metapaths\n",
    "    def dfs(current_type, path, metapaths):\n",
    "        if len(path) >= max_depth:\n",
    "            return\n",
    "\n",
    "        for rel, next_type in graph[current_type]:\n",
    "            # Avoid simple cycles (same node type reappearing)\n",
    "            if next_type in [t for (_, _, t) in path]:\n",
    "                continue\n",
    "\n",
    "            new_path = path + [(current_type, rel, next_type)]\n",
    "            metapaths.append(new_path)\n",
    "            dfs(next_type, new_path, metapaths)\n",
    "\n",
    "    # Step 3: Run DFS from each node type\n",
    "    all_metapaths = {}\n",
    "    for node_type in data.node_types:\n",
    "        metapaths = []\n",
    "        dfs(node_type, [], metapaths)\n",
    "        all_metapaths[node_type] = metapaths\n",
    "\n",
    "    return all_metapaths\n",
    "\n",
    "\n",
    "metapaths = get_metapaths_from_heterodata(data, max_depth=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4fc2e75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Dataset', 'DataCenter', 'Project', 'Platform', 'Instrument', 'Publication', 'ScienceKeyword'])\n",
      "\n",
      "metapaths[Dataset] : [[('Dataset', 'HAS_PLATFORM', 'Platform')], [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')], [('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')], [('Dataset', 'OF_PROJECT', 'Project')]]\n",
      "metapaths[DataCenter] : [[('DataCenter', 'HAS_DATASET', 'Dataset')], [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'HAS_PLATFORM', 'Platform')], [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')], [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')], [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project')]]\n",
      "metapaths[Project] : []\n",
      "metapaths[Platform] : [[('Platform', 'HAS_INSTRUMENT', 'Instrument')]]\n",
      "metapaths[Instrument] : []\n",
      "metapaths[Publication] : [[('Publication', 'USES_DATASET', 'Dataset')], [('Publication', 'USES_DATASET', 'Dataset'), ('Dataset', 'HAS_PLATFORM', 'Platform')], [('Publication', 'USES_DATASET', 'Dataset'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')], [('Publication', 'USES_DATASET', 'Dataset'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')], [('Publication', 'USES_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project')]]\n",
      "metapaths[ScienceKeyword] : [[('ScienceKeyword', 'SUBCATEGORY_OF', 'ScienceKeyword')]]\n"
     ]
    }
   ],
   "source": [
    "print(metapaths.keys())\n",
    "#print(metapaths)\n",
    "print()\n",
    "for k,v in metapaths.items():\n",
    "    print(f\"metapaths[{k}] : {metapaths[k]}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce5ee9",
   "metadata": {},
   "source": [
    "Idea: Metapath will be linking Publications that use Datasets that are of Projects. If we can predict links between Publications and Projects that we think would be linked, then we can connect author's of the publications with new Projects that should already be linked due to similar fields/common interests. <br>\n",
    "Publication USES_DATASET Dataset, Dataset OF_PROJECT Project <br>\n",
    "Could also try ('Publication', 'USES_DATASET', 'Dataset'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword') ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b34057",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/2.6.0/tutorial/shallow_node_embeddings.\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "721d7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Embeddings using MetaPath2Vec package from PyG\n",
    "\n",
    "from torch_geometric.nn import MetaPath2Vec\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "metapath = [('Publication', 'USES_DATASET', 'Dataset'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
    "\n",
    "model = MetaPath2Vec(data.edge_index_dict, embedding_dim=128,\n",
    "                     metapath=metapath, walk_length=3, context_size=2,\n",
    "                     walks_per_node=5, num_negative_samples=5,\n",
    "                     sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=6)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_steps=100, eval_steps=2000):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                  f'Loss: {total_loss / log_steps:.4f}')\n",
    "            total_loss = 0\n",
    "\n",
    "        if (i + 1) % eval_steps == 0:\n",
    "            acc = test()\n",
    "            print(f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                  f'Acc: {acc:.4f}')\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(train_ratio=0.1):\n",
    "    model.eval()\n",
    "\n",
    "    z = model('author', batch=data['author'].y_index.to(device))\n",
    "    y = data['author'].y\n",
    "\n",
    "    perm = torch.randperm(z.size(0))\n",
    "    train_perm = perm[:int(z.size(0) * train_ratio)]\n",
    "    test_perm = perm[int(z.size(0) * train_ratio):]\n",
    "\n",
    "    return model.test(z[train_perm], y[train_perm], z[test_perm], y[test_perm],\n",
    "                      max_iter=150)\n",
    "\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(epoch)\n",
    "    acc = test()\n",
    "    print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376f921",
   "metadata": {},
   "source": [
    "### Method 2: Alternative Approach \n",
    "Task: Choose and implement another link prediction method. ○ Description: This method should not use embeddings. You can use any approach of your choice. Compare the performance of this method with the embedding-based method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
