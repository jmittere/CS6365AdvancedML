{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ad058d",
   "metadata": {},
   "source": [
    "# Content-Based Recommender on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b96bb",
   "metadata": {},
   "source": [
    "This approach is using content-based for every recommendation regardless of whether or not it is a cold start user or warm user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb1d1d",
   "metadata": {},
   "source": [
    "Going to use SBERT to generate embeddings of Books and User History Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d139777a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>0593235657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1640629604904</td>\n",
       "      <td>1446304000 1564770672 1442450703 1780671067 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGKASBHYZPGTEPO6LWZPVJWB2BVA</td>\n",
       "      <td>0803736800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1454676557000</td>\n",
       "      <td>0811849783 0803729952 0735336296 1508558884 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGXFEGMNVCSTSYYA5UWXDV7AFSXA</td>\n",
       "      <td>1542046599</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1605649719611</td>\n",
       "      <td>1578052009 1477493395 1594747350 1594749310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFWHJ6O3PV4JC7PVOJH6CPULO2KQ</td>\n",
       "      <td>0679450815</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1638987703546</td>\n",
       "      <td>B00INIQVJA 1496407903 1974633225 B07KD27RHM 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHXBL3QDWZGJYH7A5CMPFNUPMF7Q</td>\n",
       "      <td>1250866448</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1669414969335</td>\n",
       "      <td>0920668372 1589255208 2764322836 2764330898 00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id parent_asin  rating      timestamp  \\\n",
       "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  0593235657     5.0  1640629604904   \n",
       "1  AGKASBHYZPGTEPO6LWZPVJWB2BVA  0803736800     4.0  1454676557000   \n",
       "2  AGXFEGMNVCSTSYYA5UWXDV7AFSXA  1542046599     5.0  1605649719611   \n",
       "3  AFWHJ6O3PV4JC7PVOJH6CPULO2KQ  0679450815     5.0  1638987703546   \n",
       "4  AHXBL3QDWZGJYH7A5CMPFNUPMF7Q  1250866448     5.0  1669414969335   \n",
       "\n",
       "                                             history  \n",
       "0  1446304000 1564770672 1442450703 1780671067 16...  \n",
       "1  0811849783 0803729952 0735336296 1508558884 08...  \n",
       "2        1578052009 1477493395 1594747350 1594749310  \n",
       "3  B00INIQVJA 1496407903 1974633225 B07KD27RHM 16...  \n",
       "4  0920668372 1589255208 2764322836 2764330898 00...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "book_test_df = pd.read_csv('../data/Books.test.csv.gz', compression='gzip', sep=',', header=0)\n",
    "book_val_df = pd.read_csv('../data/Books.valid.csv.gz', compression='gzip', sep=',', header=0)\n",
    "book_train_df = pd.read_csv('../data/Books.train.csv.gz', compression='gzip', sep=',', header=0)\n",
    "\n",
    "book_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f02abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Books in train, test and val set:  495063\n"
     ]
    }
   ],
   "source": [
    "#Combine into a list of all unique parent_asins for filtering metadata file\n",
    "unique_test = book_test_df['parent_asin'].unique()\n",
    "unique_val = book_val_df['parent_asin'].unique()\n",
    "unique_train = book_train_df['parent_asin'].unique()\n",
    "unique_items = np.concatenate([unique_train, unique_val, unique_test])\n",
    "unique_items = list(set(unique_items))\n",
    "print(\"Unique Books in train, test and val set: \", len(unique_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0cb25",
   "metadata": {},
   "source": [
    "Filter metadata file to only books that are used in train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9987de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 495063 filtered book records to: ../data/meta_Books_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "#filter down to only books in train, test, val sets\n",
    "import gzip, json\n",
    "\n",
    "metadata_file = '../data/meta_Books.jsonl.gz'\n",
    "output_filtered = '../data/meta_Books_filtered.jsonl'\n",
    "\n",
    "unique_items_set = set(unique_items)\n",
    "\n",
    "count = 0\n",
    "with gzip.open(metadata_file, \"rt\", encoding=\"utf-8\") as src, \\\n",
    "     open(output_filtered, \"w\", encoding=\"utf-8\") as dst:\n",
    "    for line in src:\n",
    "        item = json.loads(line)\n",
    "        asin = item.get(\"parent_asin\")\n",
    "        if asin in unique_items_set:\n",
    "            dst.write(line)\n",
    "            count += 1\n",
    "\n",
    "print(\"Wrote\", count, \"filtered book records to:\", output_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40037b7c",
   "metadata": {},
   "source": [
    "Loading all the Book Metadata safely - the Book metadata compressed file is 4.6 GB. <br>\n",
    "Generating Book Embeddings while loading in the data for each of the books in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06be3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets embedding for a single book\n",
    "def encode_book(model, text):\n",
    "    return model.encode(text)\n",
    "\n",
    "#limits number of words passed to encoder for a given sequence of text\n",
    "def truncate_words(text, max_words=200):\n",
    "    words = text.split()\n",
    "    return \" \".join(words[:max_words])\n",
    "\n",
    "#Builds text that will be passed to encoder to get book embedding\n",
    "def parse_book_metadata(item):\n",
    "    if('title' in item and item['title'] and item['title'] != []): #checks key exist, isnt None and is not empty list\n",
    "        title = item['title']\n",
    "    else:\n",
    "        title = \"\"\n",
    "    \n",
    "    if('categories' in item and item['categories'] and item['categories'] != []):\n",
    "        categories = \" \".join(str(x) for x in item['categories'])\n",
    "    else:\n",
    "        categories = \"\"\n",
    "    \n",
    "    if('author' in item and item['author']):\n",
    "        if('name' in item['author'] and item['author']['name'] and item['author']['name'] != []):\n",
    "            author = item['author']['name']\n",
    "        else:\n",
    "            author = \"\"\n",
    "    else:\n",
    "        author = \"\"\n",
    "    \n",
    "    if('features' in item and item['features'] and item['features']!= []):\n",
    "        features_str = \" \".join(str(x) for x in item['features'])\n",
    "        features = truncate_words(features_str, max_words=50)\n",
    "    else:\n",
    "        features = \"\"\n",
    "\n",
    "    if('description' in item and item['description'] and item['description']!= []):\n",
    "        desc_str = \" \".join(str(x) for x in item['description'])\n",
    "        description = truncate_words(desc_str, max_words=50)\n",
    "    else:\n",
    "        description = \"\"\n",
    "    \n",
    "    return title, categories, f\"{title} {categories} {author} {description}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6a06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Torch version: 2.9.1+cu128\n",
      "Built with CUDA: 12.8\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Built with CUDA:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb24cb",
   "metadata": {},
   "source": [
    "Note: This code will take a while. We are embedding all the books used across the train, test and val sets which is 495,063 books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding books:  81%|████████  | 400000/495063 [13:17<16:14, 97.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 0 with 400000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding books: 100%|██████████| 495063/495063 [16:27<00:00, 501.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final chunk 1 with 95063 rows\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Config\n",
    "filtered_file = '../data/meta_Books_filtered.jsonl'\n",
    "output_dir = '../data/book_embeddings_chunks'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "batch_size = 128\n",
    "save_every = 100_000\n",
    "\n",
    "# Load Model\n",
    "model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "book_rows = []\n",
    "texts_batch = []\n",
    "meta_batch = []\n",
    "chunk_id = 0\n",
    "processed = 0\n",
    "\n",
    "with open(filtered_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=len(unique_items), desc=\"Encoding books\"):\n",
    "        item = json.loads(line)\n",
    "        asin = item[\"parent_asin\"]\n",
    "\n",
    "        # Build text input\n",
    "        title, categories, item_str = parse_book_metadata(item)\n",
    "\n",
    "        # Add to batch\n",
    "        texts_batch.append(item_str)\n",
    "        meta_batch.append((asin, title, categories))\n",
    "\n",
    "        # Encode when batch full\n",
    "        if len(texts_batch) >= batch_size:\n",
    "            embeddings = model.encode(\n",
    "                texts_batch,\n",
    "                convert_to_numpy=True,\n",
    "                batch_size=batch_size,\n",
    "                normalize_embeddings=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "\n",
    "            for i, (asin, title, categories) in enumerate(meta_batch):\n",
    "                book_rows.append({\n",
    "                    \"parent_asin\": asin,\n",
    "                    \"title\": title,\n",
    "                    \"categories\": categories,\n",
    "                    \"embedding\": embeddings[i],\n",
    "                })\n",
    "\n",
    "            processed += len(texts_batch)\n",
    "            texts_batch.clear()\n",
    "            meta_batch.clear()\n",
    "\n",
    "            # Save chunks\n",
    "            if processed % save_every == 0:\n",
    "                df = pd.DataFrame(book_rows)\n",
    "                df.to_pickle(f\"{output_dir}/chunk_{chunk_id}.pkl\")\n",
    "                print(f\"Saved chunk {chunk_id} with {len(book_rows)} rows\")\n",
    "                book_rows.clear()\n",
    "                chunk_id += 1\n",
    "\n",
    "# encode trailing batch left\n",
    "if texts_batch:\n",
    "    embeddings = model.encode(\n",
    "        texts_batch,\n",
    "        convert_to_numpy=True,\n",
    "        batch_size=batch_size,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    for i, (asin, title, categories) in enumerate(meta_batch):\n",
    "        book_rows.append({\n",
    "            \"parent_asin\": asin,\n",
    "            \"title\": title,\n",
    "            \"categories\": categories,\n",
    "            \"embedding\": embeddings[i],\n",
    "        })\n",
    "\n",
    "# save to pickle files\n",
    "df = pd.DataFrame(book_rows)\n",
    "df.to_pickle(f\"{output_dir}/chunk_{chunk_id}.pkl\")\n",
    "print(f\"Saved final chunk {chunk_id} with {len(book_rows)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6d0287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>0593235657</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1640629604904</td>\n",
       "      <td>1446304000 1564770672 1442450703 1780671067 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGKASBHYZPGTEPO6LWZPVJWB2BVA</td>\n",
       "      <td>0803736800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1454676557000</td>\n",
       "      <td>0811849783 0803729952 0735336296 1508558884 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGXFEGMNVCSTSYYA5UWXDV7AFSXA</td>\n",
       "      <td>1542046599</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1605649719611</td>\n",
       "      <td>1578052009 1477493395 1594747350 1594749310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFWHJ6O3PV4JC7PVOJH6CPULO2KQ</td>\n",
       "      <td>0679450815</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1638987703546</td>\n",
       "      <td>B00INIQVJA 1496407903 1974633225 B07KD27RHM 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHXBL3QDWZGJYH7A5CMPFNUPMF7Q</td>\n",
       "      <td>1250866448</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1669414969335</td>\n",
       "      <td>0920668372 1589255208 2764322836 2764330898 00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id parent_asin  rating      timestamp  \\\n",
       "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  0593235657     5.0  1640629604904   \n",
       "1  AGKASBHYZPGTEPO6LWZPVJWB2BVA  0803736800     4.0  1454676557000   \n",
       "2  AGXFEGMNVCSTSYYA5UWXDV7AFSXA  1542046599     5.0  1605649719611   \n",
       "3  AFWHJ6O3PV4JC7PVOJH6CPULO2KQ  0679450815     5.0  1638987703546   \n",
       "4  AHXBL3QDWZGJYH7A5CMPFNUPMF7Q  1250866448     5.0  1669414969335   \n",
       "\n",
       "                                             history  \n",
       "0  1446304000 1564770672 1442450703 1780671067 16...  \n",
       "1  0811849783 0803729952 0735336296 1508558884 08...  \n",
       "2        1578052009 1477493395 1594747350 1594749310  \n",
       "3  B00INIQVJA 1496407903 1974633225 B07KD27RHM 16...  \n",
       "4  0920668372 1589255208 2764322836 2764330898 00...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5c439",
   "metadata": {},
   "source": [
    "Load Book Embeddings from ../data/book_embeddings_chunks folder. Embeddings include all books in train, test and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2698da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "embedding_dir = \"../data/book_embeddings_chunks\"\n",
    "files = sorted(glob.glob(f\"{embedding_dir}/chunk_*.pkl\"))\n",
    "\n",
    "asin2emb = {}\n",
    "\n",
    "# Load each chunk and populate dictionary\n",
    "for f in files:\n",
    "    df = pd.read_pickle(f)\n",
    "    for row in df.itertuples():\n",
    "        asin2emb[row.parent_asin] = row.embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12894060",
   "metadata": {},
   "source": [
    "Get embeddings for each user in test set that are the average of the embeddings for each book in the user's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21052300",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_test_df[\"history\"] = book_test_df[\"history\"].fillna(\"\")\n",
    "book_test_df['history'] = book_test_df['history'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_embeddings = {}\n",
    "\n",
    "for user, rows in book_test_df.groupby(\"user_id\"):\n",
    "    history = rows[\"history\"].sum()  # concatenates all lists of books in history\n",
    "    history_embs = []\n",
    "    for book_id in history: \n",
    "        if book_id in asin2emb: # if we have embedding for this book\n",
    "            history_embs.append(asin2emb[book_id])\n",
    "\n",
    "    if len(history_embs) == 0: #no books in user history\n",
    "        continue\n",
    "\n",
    "    user_emb = np.mean(history_embs, axis=0)\n",
    "    user_emb = user_emb / np.linalg.norm(user_emb) #normalize user embds\n",
    "    test_user_embeddings[user] = user_emb  #gets average of the embeddings of all books in test user's history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f64b8",
   "metadata": {},
   "source": [
    "Only Test Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items in test set\n",
    "test_asins = set(book_test_df['parent_asin'])\n",
    "test_asins = [a for a in test_asins if a in asin2emb]  #keep only ASINs with embeddings\n",
    "\n",
    "#Create test item matrix\n",
    "test_item_matrix = np.vstack([asin2emb[a] for a in test_asins])  # shape: (N_test_items, embedding_dim)\n",
    "test_item_matrix = test_item_matrix.T  # transpose for dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "491e5448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Test User Embeddings:  776370\n",
      "Item Matrix Shape:  (768, 256252)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Test User Embeddings: \" , len(test_user_embeddings))\n",
    "print(\"Item Matrix Shape: \", test_item_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33796644",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_purchased = {}\n",
    "\n",
    "for _, row in book_test_df.iterrows():\n",
    "    user = row[\"user_id\"]\n",
    "    purchased_book = row['parent_asin']\n",
    "    test_purchased[user] = purchased_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d18f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(ranked_items, purchased_item, k):\n",
    "    \"\"\"Compute nDCG@k for a single user (binary relevance)\"\"\"\n",
    "    try:\n",
    "        rank = ranked_items[:k].index(purchased_item) + 1\n",
    "        return 1.0 / np.log2(rank + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def hit_at_k(ranked_items, purchased_item, k):\n",
    "    \"\"\"Hit rate@k for a single user\"\"\"\n",
    "    return int(purchased_item in ranked_items[:k])\n",
    "\n",
    "def mrr_score(ranked_items, purchased_item):\n",
    "    \"\"\"Mean Reciprocal Rank for a single user\"\"\"\n",
    "    if purchased_item in ranked_items:\n",
    "        rank = ranked_items.index(purchased_item) + 1\n",
    "        return 1.0 / rank\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def auc_score(scores, purchased_idx, num_samples=1000):\n",
    "    \"\"\"Approximate AUC using random sampled negatives\"\"\"\n",
    "    num_items = len(scores)\n",
    "    neg_idx = np.random.choice(\n",
    "        [j for j in range(num_items) if j != purchased_idx],\n",
    "        size=min(num_samples, num_items-1),\n",
    "        replace=False\n",
    "    )\n",
    "    return np.mean(scores[purchased_idx] > scores[neg_idx])\n",
    "\n",
    "def get_topk_indices(scores, k):\n",
    "    \"\"\"Return indices of top-k scores\"\"\"\n",
    "    idx = np.argpartition(-scores, k)[:k]\n",
    "    return idx[np.argsort(scores[idx])[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648517c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sampled users on GPU: 100%|██████████| 10/10 [05:02<00:00, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics (sampled users):\n",
      "HitRate@5: 0.0116\n",
      "HitRate@10: 0.0237\n",
      "nDCG@5: 0.0067\n",
      "nDCG@10: 0.0106\n",
      "MRR: 0.0080\n",
      "AUC: 0.7757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "device = \"cuda\"\n",
    "batch_size = 1024\n",
    "top_k = 100\n",
    "auc_sample_size = 1000\n",
    "sample_size = 10000  # number of users to sample\n",
    "\n",
    "# Sample users\n",
    "all_users = list(test_user_embeddings.keys())\n",
    "sampled_users = random.sample(all_users, min(sample_size, len(all_users)))\n",
    "user_ids = sampled_users\n",
    "user_matrix = np.vstack([test_user_embeddings[u] for u in sampled_users])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "user_matrix = torch.tensor(user_matrix, dtype=torch.float32, device=device)\n",
    "test_item_matrix = torch.tensor(test_item_matrix, dtype=torch.float32, device=device)\n",
    "\n",
    "num_users = user_matrix.size(0)\n",
    "num_items = test_item_matrix.size(0)\n",
    "\n",
    "# Metric accumulators\n",
    "hit5_list, hit10_list = [], []\n",
    "ndcg5_list, ndcg10_list = [], []\n",
    "mrr_list, auc_list = [], []\n",
    "\n",
    "# Batched evaluation\n",
    "for start_idx in tqdm(range(0, num_users, batch_size), desc=\"Evaluating sampled users on GPU\"):\n",
    "    end_idx = min(start_idx + batch_size, num_users)\n",
    "    user_batch = user_matrix[start_idx:end_idx]\n",
    "    user_ids_batch = user_ids[start_idx:end_idx]\n",
    "\n",
    "    # Dot product -> (batch_size, num_items)\n",
    "    scores_batch = torch.matmul(user_batch, test_item_matrix) #get similarity scores between sampled user and test items\n",
    "    scores_batch_cpu = scores_batch.cpu().numpy()\n",
    "\n",
    "    for i, user in enumerate(user_ids_batch):\n",
    "        scores = scores_batch_cpu[i]\n",
    "        purchased_item = test_purchased[user]\n",
    "        purchased_idx = test_asins.index(purchased_item)\n",
    "\n",
    "        # Top-k indices\n",
    "        topk_idx = get_topk_indices(scores, top_k)\n",
    "        topk_asins = [test_asins[j] for j in topk_idx]\n",
    "\n",
    "        # Compute metrics\n",
    "        hit10_list.append(hit_at_k(topk_asins, purchased_item, 10))\n",
    "        hit5_list.append(hit_at_k(topk_asins, purchased_item, 5))\n",
    "        ndcg10_list.append(ndcg_at_k(topk_asins, purchased_item, 10))\n",
    "        ndcg5_list.append(ndcg_at_k(topk_asins, purchased_item, 5))\n",
    "        mrr_list.append(mrr_score(topk_asins, purchased_item))\n",
    "        auc_list.append(auc_score(scores, purchased_idx, num_samples=auc_sample_size))\n",
    "\n",
    "# Aggregate metrics\n",
    "metrics = {\n",
    "    \"HitRate@5\": np.mean(hit5_list),\n",
    "    \"HitRate@10\": np.mean(hit10_list),\n",
    "    \"nDCG@5\": np.mean(ndcg5_list),\n",
    "    \"nDCG@10\": np.mean(ndcg10_list),\n",
    "    \"MRR\": np.mean(mrr_list),\n",
    "    \"AUC\": np.mean(auc_list)\n",
    "}\n",
    "\n",
    "print(\"Evaluation Metrics (sampled users):\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
